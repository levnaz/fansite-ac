# Table of Contents
1. [Introduction](README.md#introduction)
2. [Details of Implementation](README.md#details-of-implementation)
3. [Repo directory structure](README.md#repo-directory-structure)


# Introduction
This program is designed to analyze a large amount of Internet traffic data generated by a NASA fan website. The challenge is to perform basic analytics on the server log file. In particular, the program has to support the following features:

**Feature 1:** List the top 10 most active host/IP addresses that have accessed the site.

**Feature 2:** Identify the 10 resources that consume the most bandwidth on the site.

**Feature 3:** List the top 10 busiest (or most frequently visited) 60-minute periods.

**Feature 4:** Detect patterns of three failed login attempts from the same IP address over 20 seconds so that all further attempts to the site can be blocked for 5 minutes. Log those possible security breaches.

It was also required that the program does not take long to run.


## Details of Implementation
I implemented a few helper functions to make the main program's code clean, easy to read and organized. The functions are:

  - `get_time()`: Takes a log file `line` and a `time format` (optional) in as input arguments and returns the `line`'s timestamp field as a `datetime` object.
  - `datetime_to_string()`: Takes a `datetime` object and a `time format` (optional) in as input arguments and returns a string.
  - `timestr_to_datetime()`: Takes a time as a string and a `time format` (optional) in as input arguments and returns a `datetime` object.
  - `get_time_delta()`: Takes two `datetime` objects and a `time format` (optional) in as input arguments and returns the time difference in seconds.
  - `add_minutes()`: Takes a `datetime` object and desired minutes (optional) in as input arguments, adds minutes to the given time, and returns a `datetime` object.
  - `add_seconds()`: Similar to `add_minutes()`, but adds seconds to the given `datetime` object. Returns a `datetime` object.
  - `get_host()`: Takes a log file `line` in as an input argument and returns the hostname/IP address field.
  - `get_reply_code(line)`: Takes a log file `line` in as an input argument and returns the HTTP reply code field (e.g. 200 or 401).
  - `get_resource(line)`: Takes a log file `line` in as an input argument and returns the resource field of the `line`.
  - `get_resource_regex()`: Similar to `get_resource()`, but uses `regex` to process the `line`.
  - `get_bytes()`: Takes a log file `line` in as an input argument and returns the bytes field of the `line`. Some lines in the log list have the `-` character in the bytes field. Interpreted `-` as 0 bytes.
  - `main()`: Reads input arguments and calls methods for all 4 challenges. Also measures the running time of each method.


### Feature 1 
List in descending order the top 10 most active hosts/IP addresses that have accessed the site.

To solve this problem, a dictionary is used to store `hosts` as `keys` and `frequencies` as `values`. The `get_host()` function is called on each `line` read. After one pass, the dictionary is constructed. `Counter` is used to find the top 10 most active host/IP addresses. The results are written in a file called `hosts.txt`.

This function takes ~10 seconds to run on the provided `log.txt` file (425.8 MB, 4,400,644 lines). The contents of the `host.txt`:

    piweba3y.prodigy.com,22309
    piweba4y.prodigy.com,14903
    piweba1y.prodigy.com,12876
    siltb10.orl.mmc.com,10578
    alyssa.prodigy.com,10184
    edams.ksc.nasa.gov,9095
    piweba2y.prodigy.com,7961
    163.206.89.4,6520
    www-d3.proxy.aol.com,6299
    vagrant.vf.mmc.com,6096


### Feature 2 
Identify the top 10 resources on the site that consume the most bandwidth. Bandwidth consumption can be extrapolated from bytes sent over the network and the frequency by which they were accessed.

To solve this problem, a dictionary is used to store `resource` as `keys` and `bytes` as `values`. The `get_resource()` and `get_bytes()` functions are called to get the required fields from a `line`. `Counter` is used to find the top 10 resources that consume the most bandwidth. The results are written in a file called `resources.txt`.

This function takes ~25 seconds to run on the provided `log.txt` file (425.8 MB, 4,400,644 lines). The contents of the `resources.txt`:

    /shuttle/missions/sts-71/movies/sts-71-launch.mpg
    /
    /shuttle/missions/sts-71/movies/sts-71-tcdt-crew-walkout.mpg
    /shuttle/missions/sts-53/movies/sts-53-launch.mpg
    /shuttle/countdown/count70.gif
    /shuttle/missions/sts-71/movies/sts-71-hatch-hand-group.mpg
    /shuttle/technology/sts-newsref/stsref-toc.html
    /shuttle/countdown/video/livevideo2.gif
    /shuttle/countdown/count.gif
    /shuttle/countdown/video/livevideo.gif



### Feature 3 
List in descending order the site's 10 busiest (i.e. most frequently visited) 60-minute periods.

To solve this problem, 3 dictionaries are used. The first dictionary stores `timestamp` as `keys` and `number_of_accesses` as `values`. The second dictionary stores `timestamp` as `keys` and `number_of_accesses` as `values` for 60-minute long windows. The third dictionary is used to store the information, which later should be analyzed and written in a file named `hours.txt`.

`access_time()` is used to get the `timestamps`, which are counted and stored in the first dictionary. Then the earliest time stamp of the first dictionary is found and a 60-minute long window is created. The number of accesses within the 60-minute long time window is counted and stored as a `value`. All timestamps starting from the earliest are processed and the results are stored in `buckets`. The following algorithm is used to count each bucket's value:

    current_bucket = previous_backet
                   - previous_backet_without_its_first_second
                   + current_bucket's_last_second_only
    # Move the current window by 1 second in each loop.

The third dictionary is constructed by filtering necessary key-value pairs from the second (buckets) dictionary. `Counter` is used to find the top 10 busiest 60-minute periods. The results are written in a file called `hours.txt`.

This function takes ~130 seconds to run on the provided `log.txt` file (425.8 MB, 4,400,644 lines). The contents of the `hours.txt`:

    01/Jul/1995:10:32:16 -0400,8423
    01/Jul/1995:10:33:37 -0400,8422
    01/Jul/1995:10:32:20 -0400,8421
    01/Jul/1995:10:32:17 -0400,8421
    01/Jul/1995:10:28:26 -0400,8421
    01/Jul/1995:10:33:34 -0400,8421
    01/Jul/1995:10:33:38 -0400,8421
    01/Jul/1995:10:32:14 -0400,8421
    01/Jul/1995:10:33:31 -0400,8420
    01/Jul/1995:10:33:35 -0400,8417


### Feature 4 
Detect patterns of three consecutive failed login attempts over 20 seconds in order to block all further attempts to reach the site from the same IP address for the next 5 minutes. Each attempt that would have been blocked should be written to a log file named `blocked.txt`.

To solve this problem, 2 dictionaries are used. The first dictionary stores `hostname` as `keys` and `number_of_failed_attempts` as `values`. The second *ordered* dictionary (`OrderedDict`) stores `timestamp` as `keys` and the corresponding `line` from the log file as `value`. After detecting 3 failed attempts from the same IP address/hostname over a consecutive 20 seconds, all following failed login attempts within 5 minutes are written in a file called `blocked.txt`. If an IP address/hostname has not reached three failed login attempts during the 20 second window, or a login attempt that succeeds during that time period resets the failed login counter and the 20-second clock.

This function takes ~140 seconds to run on the provided `log.txt` file (425.8 MB, 4,400,644 lines). The beginning part of the `blocked.txt` contains:

    215.298.34.27 - - [01/Jul/1995:03:27:25 -0400] "POST /login HTTP/1.0" 401 1420
    218.198.17.86 - - [01/Jul/1995:03:27:51 -0400] "POST /login HTTP/1.0" 401 1420
    222.168.34.62 - - [01/Jul/1995:03:36:13 -0400] "POST /login HTTP/1.0" 401 1420
    213.140.44.17 - - [01/Jul/1995:04:46:48 -0400] "POST /login HTTP/1.0" 401 1420
    216.279.26.53 - - [01/Jul/1995:05:34:05 -0400] "POST /login HTTP/1.0" 401 1420
    217.299.85.40 - - [01/Jul/1995:07:23:49 -0400] "POST /login HTTP/1.0" 401 1420
    212.158.34.50 - - [01/Jul/1995:09:12:30 -0400] "POST /login HTTP/1.0" 401 1420
    217.160.92.44 - - [01/Jul/1995:10:57:17 -0400] "POST /login HTTP/1.0" 401 1420
    221.299.29.52 - - [01/Jul/1995:11:29:31 -0400] "POST /login HTTP/1.0" 401 1420
    207.125.54.37 - - [01/Jul/1995:20:50:15 -0400] "POST /login HTTP/1.0" 401 1420
    218.213.89.72 - - [01/Jul/1995:21:54:40 -0400] "POST /login HTTP/1.0" 401 1420
    216.219.22.45 - - [02/Jul/1995:02:55:37 -0400] "POST /login HTTP/1.0" 401 1420
    210.246.51.43 - - [02/Jul/1995:03:10:44 -0400] "POST /login HTTP/1.0" 401 1420
    210.198.76.12 - - [02/Jul/1995:03:43:25 -0400] "POST /login HTTP/1.0" 401 1420
    217.224.88.13 - - [02/Jul/1995:03:56:37 -0400] "POST /login HTTP/1.0" 401 1420
    208.225.61.45 - - [02/Jul/1995:05:07:42 -0400] "POST /login HTTP/1.0" 401 1420
    223.107.92.53 - - [02/Jul/1995:05:42:29 -0400] "POST /login HTTP/1.0" 401 1420
    215.223.31.22 - - [02/Jul/1995:05:50:44 -0400] "POST /login HTTP/1.0" 401 1420
    211.157.59.79 - - [02/Jul/1995:06:24:53 -0400] "POST /login HTTP/1.0" 401 1420
    216.172.27.88 - - [02/Jul/1995:06:31:05 -0400] "POST /login HTTP/1.0" 401 1420
    219.118.73.56 - - [02/Jul/1995:06:37:36 -0400] "POST /login HTTP/1.0" 401 1420
    217.190.65.92 - - [02/Jul/1995:06:46:20 -0400] "POST /login HTTP/1.0" 401 1420
    ...


## Repo Directory Structure

The directory structure for my repo looks like this:

    ├── README.md
    ├── run.sh
    ├── src
    │   └── process_log.py
    ├── log_input
    │   └── log.txt
    ├── log_output
    |   └── hosts.txt
    |   └── hours.txt
    |   └── resources.txt
    |   └── blocked.txt
    ├── insight_testsuite
        └── run_tests.sh
        └── tests
            └── test_features
            |   ├── log_input
            |   │   └── log.txt
            |   |__ log_output
            |   │   └── hosts.txt
            |   │   └── hours.txt
            |   │   └── resources.txt
            |   │   └── blocked.txt
            ├── your-own-test
                ├── log_input
                │   └── your-own-log.txt
                |__ log_output
                    └── hosts.txt
                    └── hours.txt
                    └── resources.txt
                    └── blocked.txt

`run.sh` will call the program (`process_log.py`) written in Python and pass all required input arguments. The program reads the `log_input/log.txt` log file and writes the result files (see above) in `log_output` directory. Please note that the file size limit on Github is 100 MB so I will not be able to include the provided input file in my `log_input` directory. However, the data can be downloaded from:
https://drive.google.com/file/d/0B7-XWjN4ezogbUh6bUl1cV82Tnc/view

The repo also includes the provided `insight_testsuite`.
